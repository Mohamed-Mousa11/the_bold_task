# Platform Engineering Assessment – GCP / GKE Solution

This repository contains a minimal but production-oriented implementation of the **Platform Engineering Assessment** using **Google Cloud Platform (GCP)**:

- **GKE** (Google Kubernetes Engine) for the Kubernetes cluster
- **Cloud SQL for PostgreSQL** for the database
- **Google Cloud Load Balancer** via a `Service` of type `LoadBalancer`
- **Terraform** for infrastructure-as-code
- **GitHub Actions** for CI/CD

It implements **two environments** (staging and production) using:
- A **single shared GKE cluster**
- Two **Kubernetes namespaces**: `staging` and `production`
- A single **Cloud SQL instance** with two databases: `app_staging` and `app_production`

This keeps costs low while still providing clear separation between environments.

---

## 1. Architecture Overview

### High‑level diagram

```mermaid
graph TD
  User[User] -->|HTTP/HTTPS| GCLB[Google Cloud Load Balancer]
  GCLB --> K8sSvc[Service (type=LoadBalancer)]
  K8sSvc --> Pod[Demo App Pod (Node.js)]
  Pod -->|5432/tcp| CloudSQL[(Cloud SQL PostgreSQL)]
```

### Components

- **Network**
  - One custom VPC: `platform-vpc`
  - Dedicated subnet for GKE with secondary ranges for pods & services
  - VPC peering for Cloud SQL private IP (database is **not publicly accessible**)

- **Kubernetes**
  - One GKE cluster: `platform-gke`
  - Node pool with **1 small node** (cost-conscious)
  - Two namespaces:
    - `staging`
    - `production`
  - Node.js demo app deployed as a `Deployment` + `Service`
  - Liveness and readiness probes on `/healthz` and `/readyz`
  - Resource requests/limits on the container

- **Database**
  - Cloud SQL PostgreSQL (tier `db-f1-micro`)
  - Private IP only (no public IP)
  - Databases:
    - `app_staging`
    - `app_production`
  - One app user, password generated by Terraform

- **Load Balancing**
  - Kubernetes `Service` of type `LoadBalancer`
  - GKE provisions a **Google Cloud external HTTP(S) load balancer**

- **CI/CD**
  - Single GitHub Actions workflow:
    - Triggers on:
      - `staging` branch → deploys into `staging` namespace
      - `main` branch → deploys into `production` namespace
    - Stages:
      1. **Build** Docker image
      2. **Test** (simple config health check)
      3. **Scan** container image with Trivy
      4. **Deploy** to appropriate namespace

---

## 2. Setup Instructions

### 2.1 Prerequisites

Locally you will need:

- GCP project (with billing enabled)
- `gcloud` CLI (authenticated against your project)
- `terraform` >= 1.5
- `kubectl`
- A GitHub repository where you can push this code

Required GCP APIs (enable once per project):

- `compute.googleapis.com`
- `container.googleapis.com`
- `sqladmin.googleapis.com`
- `servicenetworking.googleapis.com`

Example:

```bash
PROJECT_ID="your-gcp-project-id"

gcloud services enable   compute.googleapis.com   container.googleapis.com   sqladmin.googleapis.com   servicenetworking.googleapis.com   --project "${PROJECT_ID}"
```

### 2.2 Provision infrastructure with Terraform

1. Clone the repo and change directory:

```bash
git clone <your-fork-or-repo-url>.git
cd platform-engineering-assessment-gcp/infra
```

2. Create a minimal `terraform.tfvars` (not committed to git):

```hcl
project_id = "your-gcp-project-id"
region     = "us-central1"
```

3. Initialise and apply:

```bash
terraform init
terraform apply
```

4. After apply, capture outputs (especially DB details) for GitHub secrets:

```bash
terraform output
```

Key outputs:

- `cluster_name`
- `cluster_endpoint`
- `cloudsql_private_ip`
- `db_user`
- `db_password` (sensitive – do **not** commit)
- `staging_db_name`
- `production_db_name`

5. (Optional sanity check) Connect to the cluster once from your local machine:

```bash
gcloud container clusters get-credentials platform-gke --region us-central1 --project your-gcp-project-id

kubectl get nodes
kubectl get ns
```

You should see `staging` and `production` namespaces created by Terraform.

### 2.3 Configure GitHub Actions secrets

In your GitHub repo, configure the following **Repository secrets**:

**GCP / GKE configuration**

- `GCP_PROJECT_ID` – your project ID
- `GCP_REGION` – e.g. `us-central1`
- `GKE_CLUSTER_NAME` – `platform-gke`
- `GKE_LOCATION` – same as region, e.g. `us-central1`
- `IMAGE_REGISTRY` – e.g. `gcr.io/<project-id>`
- `GCP_WORKLOAD_IDENTITY_PROVIDER` – workload identity provider string (or use a service account key if preferred)
- `GCP_SERVICE_ACCOUNT_EMAIL` – email of CI/CD service account with permissions:
  - `roles/container.admin`
  - `roles/storage.admin`
  - `roles/cloudsql.client`

(If you don't want to use workload identity, you can replace the auth step in the workflow with a key‑based approach using `GOOGLE_CREDENTIALS` instead.)

**Database values (from Terraform outputs)**

- `STAGING_DB_HOST` – `cloudsql_private_ip` output
- `STAGING_DB_NAME` – `staging_db_name` output
- `STAGING_DB_USER` – `db_user` output
- `STAGING_DB_PASSWORD` – `db_password` output
- `PROD_DB_HOST` – same private IP (shared instance)
- `PROD_DB_NAME` – `production_db_name` output
- `PROD_DB_USER` – `db_user`
- `PROD_DB_PASSWORD` – `db_password`

> Secrets are **never** committed; they are injected into the GitHub Actions workflow, which then creates/updates the corresponding Kubernetes `Secret` and `ConfigMap` in each namespace.

### 2.4 First deployment via CI/CD

1. Push code to GitHub.

2. Create a `staging` branch and push a change:

```bash
git checkout -b staging
git push origin staging
```

This triggers the workflow and deploys to the `staging` namespace.

3. Merge PR or push to `main` to trigger a **production** deployment.

4. Fetch the external IP:

```bash
kubectl get svc demo-app -n staging
# or
kubectl get svc demo-app -n production
```

Open `http://<EXTERNAL-IP>/` in a browser to see the JSON response.

---

## 3. Architecture Decisions

1. **Cloud provider: GCP + GKE**

   Chosen to align with the assessment’s GCP option and to keep Kubernetes setup straightforward using a managed GKE cluster.

2. **Single cluster, two namespaces**

   - `staging` and `production` are separated at the Kubernetes namespace level.
   - This keeps cost low (one cluster, one node pool) while still providing config isolation, separate secrets, and separate DNS endpoints if needed.

3. **Single Cloud SQL instance, two databases**

   - One **db-f1-micro** instance to minimise cost.
   - Two databases: `app_staging` and `app_production` for env separation.
   - The same DB user is used for both databases; access is controlled at the schema/database level.

4. **GitHub Actions for CI/CD**

   - Chosen over Cloud Build because it’s repo‑local, easy to review, and cloud‑agnostic.
   - The pipeline uses branches (`staging` and `main`) to decide target namespace and DB settings.

5. **Load balancer via Service(type=LoadBalancer)**

   - Leverages GKE’s native integration with Google Cloud Load Balancer.
   - No additional ingress controller complexity for this assessment.

---

## 4. Cost Optimisation

- **Smallest feasible sizes**
  - GKE node pool with a **single `e2-medium` node**.
  - Cloud SQL tier `db-f1-micro`.
- **Shared infra**
  - One cluster and one Cloud SQL instance shared by both environments.
- **No extra services**
  - No NAT gateway, VPN, or complex networking.
  - No additional monitoring stack by default (only K8s-native probes).
- **Easy teardown**

When you’re finished:

```bash
cd infra
terraform destroy
```

This removes the VPC, GKE cluster, and Cloud SQL instance and avoids ongoing costs.

---

## 5. Security Considerations

1. **Secrets management**
   - Database credentials are generated by Terraform and never committed.
   - CI/CD injects DB credentials as GitHub secrets.
   - Kubernetes `Secret` objects (`app-secrets`) are created/updated from those secrets.

2. **Network security**
   - Cloud SQL uses **private IP only** – no public IP.
   - GKE nodes communicate with Cloud SQL over the VPC using private addressing.
   - VPC is custom with explicit subnets rather than auto mode.

3. **Database access**
   - Only the GKE node pool’s network can reach the Cloud SQL private IP.
   - Single app user with limited privileges can be extended later as needed.

4. **Container security**
   - CI/CD runs Trivy image scans and fails the pipeline on **HIGH** or **CRITICAL** vulnerabilities.
   - Minimal base image (`node:20-alpine`) to reduce attack surface.

5. **IAM**
   - A dedicated CI/CD service account is used with narrowly scoped roles (container admin, storage admin, cloudsql client).

---

## 6. Troubleshooting Guide

### 6.1 Kubernetes basics

Check pod status:

```bash
kubectl get pods -n staging
kubectl describe pod <pod-name> -n staging
```

View logs:

```bash
kubectl logs deployment/demo-app -n staging
kubectl logs pod/<pod-name> -n staging
```

Check service & load balancer:

```bash
kubectl get svc -n staging
kubectl describe svc demo-app -n staging
```

### 6.2 Application health

Health endpoints:

- Liveness: `GET /healthz`
- Readiness: `GET /readyz`

From your machine (after port-forwarding):

```bash
kubectl port-forward svc/demo-app -n staging 8080:80 &
curl http://localhost:8080/healthz
curl http://localhost:8080/readyz
```

### 6.3 Database connectivity

Inside pod:

```bash
kubectl exec -it deploy/demo-app -n staging -- sh
# Inside the container:
node -e "console.log(process.env.DB_HOST, process.env.DB_NAME)"
```

Confirm that `DB_HOST` is the Cloud SQL private IP and DB name matches `app_staging` or `app_production`.

### 6.4 CI/CD failures

- **Build/test stage fails**
  - Check Node.js tests output in the GitHub Actions logs.
- **Scan stage fails**
  - Trivy found HIGH/CRITICAL vulnerabilities.
  - Either update dependencies or temporarily relax the severity threshold.
- **Deploy stage fails**
  - Check `kubectl` output in the workflow logs.
  - Ensure `GKE_CLUSTER_NAME`, `GKE_LOCATION`, and service account permissions are correct.

---

## 7. Future Improvements

If there were more time, I would extend this repo with:

- **Monitoring**
  - Deploy Prometheus + Grafana via Helm.
  - Expose app metrics (e.g. `/metrics` endpoint) and build dashboards.
- **Security hardening**
  - Network policies to restrict pod-to-pod communication.
  - Pod security standards / PSP replacement.
  - Rotate DB credentials automatically using Secrets Manager.
- **Resilience & scalability**
  - Horizontal Pod Autoscaler (HPA) based on CPU/requests.
  - Multiple node pools with different sizes for system vs workloads.
- **Release strategies**
  - Blue‑green or canary deployments (e.g. using Argo Rollouts or additional Services/Ingress).
- **Multi‑region**
  - Separate clusters per region and Cloud SQL read replicas if required.

---

## 8. Repository Layout

```text
.
├── README.md
├── app
│   ├── Dockerfile
│   ├── .dockerignore
│   ├── package.json
│   ├── src
│   │   └── index.js
│   └── scripts
│       └── healthcheck.js
├── infra
│   ├── main.tf
│   ├── variables.tf
│   ├── outputs.tf
│   ├── providers.tf
│   ├── k8s.tf
│   └── modules
│       ├── network
│       │   ├── main.tf
│       │   ├── variables.tf
│       │   └── outputs.tf
│       ├── gke
│       │   ├── main.tf
│       │   ├── variables.tf
│       │   └── outputs.tf
│       └── cloudsql
│           ├── main.tf
│           ├── variables.tf
│           └── outputs.tf
├── k8s
│   ├── deployment.yaml
│   └── service.yaml
└── .github
    └── workflows
        └── ci-cd.yaml
```

This implementation focuses on the **minimum requirements** from the assessment while still being realistic and easy to extend.
